{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Agriculture Spark Data Analysis\n",
    "\n",
    "Mit diesem Notebook werden Tierbewegungsdaten (*Hubs und Spokes*) zusammen mit Wetterdaten (Niederschlag) verarbeitet und analysiert.\n",
    "Ziel ist es, das Verhalten der Tiere in Abhängigkeit von Wetterbedingungen zu untersuchen.\n",
    "\n",
    "---\n",
    "\n",
    "##  Notebook-Struktur\n",
    "\n",
    "### 1. Datenaufbereitung *(CSV → Parquet)*\n",
    "- 1.1 Datenaufbereitung Hubs\n",
    "- 1.2 Datenaufbereitung Spokes\n",
    "- 1.3 Datenaufbereitung MeteoSchweiz Niederschlagsdaten\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Zusammenführung der Daten *(Spokes + Hubs + Meteo)*\n",
    "- 2.1 Join Spokes + Hubs\n",
    "- 2.2 Join Bewegungsdaten + Meteo\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Analyse und Auswertung der Daten\n",
    "- 3.1 Verhaltensanalyse der Tiere\n",
    "- 3.2 Aktivitätsverteilung nach Niederschlagskategorie\n",
    "- 3.3 Zeitreihenanalyse: Niederschlag und Aktivität über Zeit\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Performance-Messungen\n",
    "- Laufzeiten der einzelnen Verarbeitungsschritte\n",
    "- Gesamtverarbeitungszeit des Jobs\n"
   ],
   "id": "c9d2d3efb3bd4209"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "40e1f37f2552b463"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Datenaufbereitung (CSV → Parquet)\n",
    "In diesem Schritt werden die Rohdaten der Hubs und Spokes aus CSV-Dateien eingelesen, bereinigt und in das Parquet-Format konvertiert. Zusätzlich werden die MeteoSchweiz Niederschlagsdaten eingelesen und aufbereitet.\n",
    "\n",
    "\n"
   ],
   "id": "2e05e2783cf83707"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "\n",
    "# nur auf ZHAW-Cluster nötig\n",
    "# import swissproc\n",
    "# sc = swissproc.connect(\"frommthi\", 2)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pyspark.sql\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, date_format, to_timestamp, round, when\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "639652f19fe24a60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pfade\n",
    "\n",
    "# --- INPUT: Originaldateien ---\n",
    "HUB_CSV = \"data/raw/dm-hub.csv\"\n",
    "SPOKE_CSV = \"data/raw/dm-spoke.csv\"\n",
    "METEO_CSV = \"data/raw/ogd-smn-precip_leu_t_recent.csv\"\n",
    "\n",
    "# --- OUTPUT: Parquet-Zielordner ---\n",
    "OUT_ROOT = \"data/processed\"\n",
    "HUB_OUT_PARQUET = os.path.join(OUT_ROOT, \"hub\")\n",
    "SPOKE_OUT_PARQUET = os.path.join(OUT_ROOT, \"spoke\")\n",
    "METEO_OUT_PARQUET = os.path.join(OUT_ROOT, \"meteo\")\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"HUB_CSV:\", HUB_CSV)\n",
    "print(\"SPOKE_CSV:\", SPOKE_CSV)\n",
    "print(\"OUT_ROOT:\", OUT_ROOT)\n"
   ],
   "id": "518f0713857bec1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 Datenaufbereitung Hubs\n",
    "Die Daten der Hubs enthält folgende Spalten:\n",
    "- `hubtracker`: Eindeutige ID des Hubs\n",
    "- `timestamp_hubs`: Zeitstempel der Messung (Unix-Epoche in Sekunden -> DD.MM.YYYY HH:MM)\n",
    "- `hub_coords`: Koordinaten des Hubs als String `<latitude> <longitude>`\n",
    "- `lat_hubs`: Latitude des Hubs (Dezimalgrad)\n",
    "- `lon_hubs`: Longitude des Hubs (Dezimalgrad)\n",
    "- `voltage`: Spannung des Hubs in Volt\n",
    "- `temperature_hubs`: Temperatur des Hubs in Grad Celsius\n",
    "- `signal`: Signalstärke des Hubs (nicht benötigt)\n",
    "- `timestamp_10min`: Zeitstempel der Messung auf 10-Minuten-Intervall gerundet (wird erstellt)\n"
   ],
   "id": "89f1d19941455807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Zeit Jobstart\n",
    "start_job = time.time()"
   ],
   "id": "25bc7f0f79735038",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Zeit Datenaufbereitung\n",
    "start_datalaod = time.time()"
   ],
   "id": "a7e5a3f58e1f0d6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hub-Format: <hubtracker> <timestamp> hub-coords <latitude> <longitude> <voltage/V> <temperature/°C> <signal>\n",
    "# => 8 Spalten (signal ist die 8. Spalte, wird aber nicht benötigt)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Rohdaten einlesen\n",
    "hubs_raw = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(HUB_CSV)\n",
    "    .toDF(\n",
    "        \"hubtracker\",\n",
    "        \"timestamp_hubs\",\n",
    "        \"hub_coords\",\n",
    "        \"lat_hubs\",\n",
    "        \"lon_hubs\",\n",
    "        \"voltage\",\n",
    "        \"temperature_hubs\",\n",
    "        \"signal\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Anzahl vor Filterung\n",
    "count_before_hubs = hubs_raw.count()\n",
    "print(f\"Hubs vor Filterung: {count_before_hubs}\")\n",
    "\n",
    "# Ausreißer filtern und verarbeiten\n",
    "hubs = (\n",
    "    hubs_raw\n",
    "    .select(\"hubtracker\", \"timestamp_hubs\", \"lat_hubs\", \"lon_hubs\")\n",
    "\n",
    "    # Timestamp-Konvertierung\n",
    "    .withColumn(\n",
    "        \"timestamp_hubs_ts\",\n",
    "        from_unixtime(col(\"timestamp_hubs\")).cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # AUSREISSER-FILTERUNG: Timestamps\n",
    "    # Nur Unix-Timestamps zwischen 2025-01-01 und 2025-12-31\n",
    "    .filter(\n",
    "        (col(\"timestamp_hubs\") >= 1735689600) &  # 2025-01-01 00:00:00\n",
    "        (col(\"timestamp_hubs\") <= 1767225599)    # 2025-12-31 23:59:59\n",
    "    )\n",
    "\n",
    "    # Datentypen für Koordinaten\n",
    "    .withColumn(\"lat_hubs\", col(\"lat_hubs\").cast(\"double\"))\n",
    "    .withColumn(\"lon_hubs\", col(\"lon_hubs\").cast(\"double\"))\n",
    "\n",
    "    # AUSREISSER-FILTERUNG: Koordinaten (Schweiz: Leukerbad-Region)\n",
    "    # Latitude: 45.8 - 47.8 (Schweiz Nord-Süd)\n",
    "    # Longitude: 5.9 - 10.5 (Schweiz West-Ost)\n",
    "    .filter(\n",
    "        (col(\"lat_hubs\") >= 45.8) & (col(\"lat_hubs\") <= 47.8) &\n",
    "        (col(\"lon_hubs\") >= 5.9) & (col(\"lon_hubs\") <= 10.5)\n",
    "    )\n",
    "\n",
    "    # Timestamp auf 10-Minuten-Intervall runden\n",
    "    .withColumn(\n",
    "        \"timestamp_10min\",\n",
    "        from_unixtime(\n",
    "            (unix_timestamp(col(\"timestamp_hubs_ts\")) / 600).cast(\"long\") * 600\n",
    "        ).cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # NULL-Werte entfernen\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Anzahl nach Filterung\n",
    "count_after_hubs = hubs.count()\n",
    "print(f\"Hubs nach Filterung: {count_after_hubs}\")\n",
    "print(f\"Entfernte Ausreißer: {count_before_hubs - count_after_hubs} ({((count_before_hubs - count_after_hubs) / count_before_hubs * 100):.2f}%)\")\n",
    "\n",
    "hubs.write.mode(\"overwrite\").parquet(HUB_OUT_PARQUET)\n",
    "hubs_parquet = spark.read.parquet(HUB_OUT_PARQUET)\n",
    "\n",
    "hubs_parquet.printSchema()\n",
    "time_dataload = time.time() - start_datalaod\n",
    "print(f\"Hub-Verarbeitung: {time.time() - start:.2f} Sekunden\")\n",
    "print(f\"Anzahl Hubs: {hubs_parquet.count()}\")"
   ],
   "id": "798606240bd57c71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hubs_parquet.show(10)",
   "id": "5477ac8ea7f02eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Datenaufbereitung Spokes\n",
    "Die Daten der Spokes enthält folgende Spalten:\n",
    "- `hubtracker`: Eindeutige ID des Hubs, der den Spoke emp\n",
    "- `timestamp_spokes`: Zeitstempel der Messung (Unix-Epoche in Sekunden -> DD.MM.YYYY HH:MM)\n",
    "- `spoke_visibility`: Sichtbarkeit des Spokes (Boolean)\n",
    "- `spoketracker`: Eindeutige ID des Spokes (Tier)\n",
    "- `rssi`: Signalstärke des Spokes in dB\n",
    "- `device_state`: Zustand des Geräts (z.B. \"OK\", \"LOW_BATTERY\", etc.)\n",
    "- `voltage`: Spannung des Spokes in Volt\n",
    "- `temperature_spokes`: Temperatur des Spokes in Grad Celsius\n",
    "- `animal_state`: Zustand des Tiers (z.B. \"RESTING\", \"WALKING\", etc.)\n",
    "- `state_resting`: Zeit in Minuten, die das Tier ruhend verbracht hat\n",
    "- `state_walking`: Zeit in Minuten, die das Tier gehend verbracht hat\n",
    "- `state_grazing`: Zeit in Minuten, die das Tier grast verbracht hat\n",
    "- `state_running`: Zeit in Minuten, die das Tier rennend verbracht hat\n",
    "- `spoke_ts`: Zeitstempel der Messung als Timestamp\n",
    "- `spoke_ts_10min`: Zeitstempel der Messung auf 10-Minuten-Intervall gerundet"
   ],
   "id": "3467b65d35a4e120"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Spoke-Format: <hubtracker> <timestamp> spoke-visibility <spoketracker> <rssi/dB> <device-state>\n",
    "#               <voltage/V> <temperature/°C> <animal-state> <state-resting/min> <state-walking/min>\n",
    "#               <state-grazing/min> <state-running/min> <f01> ... <f15> <extra>\n",
    "# => 29 Spalten (15 Feature-Werte als einzelne Spalten)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Spaltennamen für alle 29 Spalten\n",
    "spoke_cols = [\n",
    "                 \"hubtracker\", \"timestamp_spokes\", \"spoke_visibility\", \"spoketracker\",\n",
    "                 \"rssi\", \"device_state\", \"voltage\", \"temperature_spokes\", \"animal_state\",\n",
    "                 \"state_resting\", \"state_walking\", \"state_grazing\", \"state_running\"\n",
    "             ] + [f\"f{i:02d}\" for i in range(1, 16)] + [\"extra\"]\n",
    "\n",
    "spokes_raw = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(SPOKE_CSV) \\\n",
    "    .toDF(*spoke_cols)\n",
    "\n",
    "# Anzahl vor Filterung\n",
    "count_before_spokes = spokes_raw.count()\n",
    "print(f\"Spokes vor Filterung: {count_before_spokes}\")\n",
    "\n",
    "# Nur benötigte Spalten selektieren (ohne extra-Spalte)\n",
    "spokes_selected = spokes_raw.select(\n",
    "    \"hubtracker\",\n",
    "    \"timestamp_spokes\",\n",
    "    \"spoke_visibility\",\n",
    "    \"spoketracker\",\n",
    "    \"rssi\",\n",
    "    \"device_state\",\n",
    "    \"voltage\",\n",
    "    \"temperature_spokes\",\n",
    "    \"animal_state\",\n",
    "    \"state_resting\",\n",
    "    \"state_walking\",\n",
    "    \"state_grazing\",\n",
    "    \"state_running\",\n",
    ")\n",
    "\n",
    "# Datentypen anpassen und Timestamp-Spalten erstellen\n",
    "spokes_cleaned = (\n",
    "    spokes_selected\n",
    "    .withColumn(\n",
    "        \"spokes_ts\",\n",
    "        from_unixtime(col(\"timestamp_spokes\")).cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # AUSREISSER-FILTERUNG: Timestamps\n",
    "    # Nur Unix-Timestamps zwischen 2025-01-01 und 2025-12-31\n",
    "    .filter(\n",
    "        (col(\"timestamp_spokes\") >= 1735689600) &  # 2025-01-01 00:00:00\n",
    "        (col(\"timestamp_spokes\") <= 1767225599)    # 2025-12-31 23:59:59\n",
    "    )\n",
    "\n",
    "    .withColumn(\n",
    "        \"spokes_ts_10min\",\n",
    "        from_unixtime(\n",
    "            (unix_timestamp(col(\"spokes_ts\")) / 600).cast(\"long\") * 600\n",
    "        ).cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # Datentypen casten\n",
    "    .withColumn(\"rssi\", col(\"rssi\").cast(\"double\"))\n",
    "    .withColumn(\"voltage\", col(\"voltage\").cast(\"double\"))\n",
    "    .withColumn(\"temperature_spokes\", col(\"temperature_spokes\").cast(\"double\"))\n",
    "    .withColumn(\"state_resting\", col(\"state_resting\").cast(\"double\"))\n",
    "    .withColumn(\"state_walking\", col(\"state_walking\").cast(\"double\"))\n",
    "    .withColumn(\"state_grazing\", col(\"state_grazing\").cast(\"double\"))\n",
    "    .withColumn(\"state_running\", col(\"state_running\").cast(\"double\"))\n",
    "\n",
    "\n",
    "    # AUSREISSER-FILTERUNG: Temperatur\n",
    "    # Realistischer Bereich für Outdoor-Sensoren: -30°C bis 50°C\n",
    "    .filter(\n",
    "        (col(\"temperature_spokes\") >= -30) & (col(\"temperature_spokes\") <= 50)\n",
    "    )\n",
    "\n",
    "\n",
    "    # NULL-Werte entfernen\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Anzahl nach Filterung\n",
    "count_after_spokes = spokes_cleaned.count()\n",
    "print(f\"Spokes nach Filterung: {count_after_spokes}\")\n",
    "print(f\"Entfernte Ausreißer: {count_before_spokes - count_after_spokes} ({((count_before_spokes - count_after_spokes) / count_before_spokes * 100):.2f}%)\")\n",
    "\n",
    "spokes_cleaned.write.mode(\"overwrite\").parquet(SPOKE_OUT_PARQUET)\n",
    "spokes_parquet = spark.read.parquet(SPOKE_OUT_PARQUET)\n",
    "\n",
    "spokes_parquet.printSchema()\n",
    "\n",
    "print(f\"Spoke-Verarbeitung: {time.time() - start:.2f} Sekunden\")\n",
    "print(f\"Anzahl Spoke-Einträge: {spokes_parquet.count()}\")\n"
   ],
   "id": "8c915599a157d9ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spokes_parquet.show(10)\n",
   "id": "7668927e4488d4c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Datenaufbereitung MeteoSchweiz Niederschlagsdaten\n",
    "Niederschlagsdaten der Station Leukerbad (CH) von MeteoSchweiz\n",
    "Quelle: https://data.geo.admin.ch/ch.meteoschweiz.ogd-smn-precip/leu/ogd-smn-precip_leu_t_recent.csv\n",
    "Die Daten enthalten folgende Spalten:\n",
    "- `station_abbr`: Stations-ID (Leukerbad: LEU)\n",
    "- `reference_timestamp`: Datum/Zeit der Messung (DD-MM-YYYY HH:MM:SS) -> Auflösung: 10 Minuten\n",
    "- `rre150z0`: Niederschlagsmenge in mm\n",
    "\n",
    "Folgende Spalten werden zusätzlich erstellt:\n",
    "- `rain_h`: Niederschlagsmenge in mm/h (berechnet aus `rain_10min` * 6)\n",
    "- `rain_category`: Kategorisierung des Niederschlags in fünf Klassen:\n",
    "    - kein (0 mm/h)\n",
    "    - sehr_schwach (0.1 - 0.5 mm/h)\n",
    "    - schwach (0.6 - 2 mm/h)\n",
    "    - mässig (2.1 - 5 mm/h)\n",
    "    - stark (5.1 - 10 mm/h)\n",
    "    - sehr_stark (> 10 mm/h)"
   ],
   "id": "67394b94afc53e0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "weather = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(METEO_CSV)\n",
    "    .toDF(\"station_id\", \"meteo_timestamp\", \"rain_10min\")\n",
    "    #Timestamp parsen\n",
    "    .withColumn(\n",
    "        \"meteo_timestamp\",\n",
    "        to_timestamp(col(\"meteo_timestamp\"), \"dd.MM.yyyy HH:mm\")\n",
    "    )\n",
    "    #auf 10 Minuten abrunden\n",
    "    .withColumn(\n",
    "        \"meteo_ts_10min\",\n",
    "        from_unixtime(\n",
    "            (unix_timestamp(col(\"meteo_timestamp\")) / 600).cast(\"long\") * 600\n",
    "        ).cast(\"timestamp\")\n",
    "    )\n",
    "    #Niederschlagsmenge in mm/h berechnen und kategorisieren\n",
    "    .withColumn(\"rain_10min\", col(\"rain_10min\").cast(\"double\"))\n",
    "    .withColumn(\"rain_h\", round(col(\"rain_10min\") * 6.0, 1))\n",
    "    .withColumn(\n",
    "        \"rain_category\",\n",
    "        when(col(\"rain_h\") == 0, \"kein\")\n",
    "        .when(col(\"rain_h\") <= 0.5, \"sehr_schwach\")\n",
    "        .when(col(\"rain_h\") <= 2, \"schwach\")\n",
    "        .when(col(\"rain_h\") <= 5, \"mässig\")\n",
    "        .when(col(\"rain_h\") <= 10, \"stark\")\n",
    "        .otherwise(\"sehr_stark\")\n",
    "    )\n",
    "    #Nur benötigte Spalten behalten\n",
    "    .drop(\"meteo_timestamp\", \"station_id\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "weather.write.mode(\"overwrite\").parquet(METEO_OUT_PARQUET)\n",
    "weather_parquet = spark.read.parquet(METEO_OUT_PARQUET)\n",
    "\n",
    "weather_parquet.printSchema()\n",
    "\n",
    "print(f\"Meteo-Verarbeitung: {time.time() - start:.2f} Sekunden\")\n",
    "print(f\"Anzahl Meteo-Einträge: {weather_parquet.count()}\")"
   ],
   "id": "585efdd3057c2bd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "weather_parquet.show(10)",
   "id": "80495bc617ed65e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Zusammenführung der Daten Spokes + Hubs + Meteo\n",
    "In diesem Schritt werden die bereinigten Daten der Spokes, Hubs und Meteo zusammengeführt, um einen umfassenden Datensatz zu erstellen, der alle relevanten Informationen für die Analyse enthält.\n",
    "\n",
    "Die Zusammenführung erfolgt in zwei Schritten:\n",
    "1. **Spokes + Hubs**: Join per `hubtracker` und `timestamp_10min` (Spokes werden den Hubs zugeordnet)\n",
    "2. **+ Meteo**: Join per `timestamp_10min` (Wetterdaten werden zeitlich zugeordnet)\n",
    "\n",
    "Da Spokes alle 5 Minuten gemessen werden, Hub und Meteo aber alle 10 Minuten, werden die Spokes-Daten auf 10-Minuten-Intervalle gerundet.\n"
   ],
   "id": "29ed0bc0d4db8737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Zeit Zusammenführung\n",
    "start_joins = time.time()"
   ],
   "id": "92c227fd69db8d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# 1. Spokes + Hubs joinen per hubtracker und 10-Minuten-Timestamp\n",
    "joined_spokes_hubs = (\n",
    "    spokes_parquet.alias(\"s\")\n",
    "    .join(\n",
    "        hubs_parquet.alias(\"h\"),\n",
    "        (col(\"s.hubtracker\") == col(\"h.hubtracker\")) &\n",
    "        (col(\"s.spokes_ts_10min\") == col(\"h.timestamp_10min\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        # Spokes-Spalten\n",
    "        col(\"s.hubtracker\"),\n",
    "        col(\"s.timestamp_spokes\"),\n",
    "        col(\"s.spokes_ts\"),\n",
    "        col(\"s.spokes_ts_10min\"),\n",
    "        col(\"s.spoke_visibility\"),\n",
    "        col(\"s.spoketracker\"),\n",
    "        col(\"s.rssi\"),\n",
    "        col(\"s.device_state\"),\n",
    "        col(\"s.voltage\").alias(\"spoke_voltage\"),\n",
    "        col(\"s.temperature_spokes\"),\n",
    "        col(\"s.animal_state\"),\n",
    "        col(\"s.state_resting\"),\n",
    "        col(\"s.state_walking\"),\n",
    "        col(\"s.state_grazing\"),\n",
    "        col(\"s.state_running\"),\n",
    "        # Hub-Spalten\n",
    "        col(\"h.lat_hubs\"),\n",
    "        col(\"h.lon_hubs\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Meteo hinzufügen per 10-Minuten-Timestamp (broadcast weil klein)\n",
    "joined_all = (\n",
    "    joined_spokes_hubs.alias(\"sh\")\n",
    "    .join(\n",
    "        broadcast(weather_parquet.alias(\"w\")),\n",
    "        col(\"sh.spokes_ts_10min\") == col(\"w.meteo_ts_10min\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"sh.*\"),\n",
    "        col(\"w.rain_10min\"),\n",
    "        col(\"w.rain_h\"),\n",
    "        col(\"w.rain_category\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Duplikate entfernen (basierend auf spoketracker, timestamp und hubtracker)\n",
    "joined_all = joined_all.dropDuplicates([\"spoketracker\", \"timestamp_spokes\", \"hubtracker\"])\n",
    "\n",
    "# Ergebnis speichern\n",
    "JOINED_OUT_PARQUET = os.path.join(OUT_ROOT, \"joined\")\n",
    "joined_all.write.mode(\"overwrite\").parquet(JOINED_OUT_PARQUET)\n",
    "joined_parquet = spark.read.parquet(JOINED_OUT_PARQUET)\n",
    "\n",
    "joined_parquet.printSchema()\n",
    "print(f\"Join-Verarbeitung: {time.time() - start:.2f} Sekunden\")\n",
    "print(f\"Anzahl Joined-Einträge (nach Deduplizierung): {joined_parquet.count()}\")\n"
   ],
   "id": "390edd489a402850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "joined_parquet.show(10, truncate=False)\n",
   "id": "1aa112fd5ad48a69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Statistik: Anzahl Einträge mit/ohne Hub-/Meteo-Daten\n",
    "print(\"=== Vollständigkeit der Daten ===\")\n",
    "total_count = joined_parquet.count()\n",
    "print(f\"Total Einträge (nach Deduplizierung): {total_count}\")\n",
    "print(f\"Mit Hub-Koordinaten: {joined_parquet.filter(col('lat_hubs').isNotNull()).count()}\")\n",
    "print(f\"Mit Meteo-Daten: {joined_parquet.filter(col('rain_10min').isNotNull()).count()}\")\n",
    "print(f\"Komplett (Spoke+Hub+Meteo): {joined_parquet.filter(col('lat_hubs').isNotNull() & col('rain_10min').isNotNull()).count()}\")\n",
    "\n",
    "# Duplikate-Check (sollte 0 sein)\n",
    "duplicate_check = joined_parquet.groupBy(\"spoketracker\", \"timestamp_spokes\", \"hubtracker\").count().filter(col(\"count\") > 1).count()\n",
    "print(f\"\\nVerbleibende Duplikate: {duplicate_check} (sollte 0 sein)\")\n",
    "\n",
    "time_joins = time.time() - start_joins\n"
   ],
   "id": "e99647bf4e6fdd3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Analyse und Auswertung der Daten\n",
    "\n",
    "### 3.1 Verhaltensanalyse der Tiere\n",
    "\n",
    "In diesem Abschnitt analysieren wir das Verhalten der Tiere anhand der erfassten Aktivitätszustände:\n",
    "- Verteilung der Aktivitätszustände (resting/walking/grazing/running)\n",
    "- Zeitliche Muster: Wann sind Tiere aktiv?\n",
    "- Korrelation zwischen Wetter und Verhalten\n"
   ],
   "id": "683af77a7a2decf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Daten für Visualisierung vorbereiten (Pandas DataFrame)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_analyzis = time.time()\n",
    "\n",
    "# Visualisierungsstil (nur Matplotlib)\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['axes.axisbelow'] = True\n",
    "\n",
    "# Joined-Daten als Pandas DataFrame laden\n",
    "#Sample für Test\n",
    "#df_viz = joined_parquet.sample(fraction=0.3, seed=42).toPandas()\n",
    "\n",
    "#Alle Daten\n",
    "df_viz = joined_parquet.toPandas()\n",
    "\n",
    "print(f\"Anzahl Datensätze für Visualisierung: {len(df_viz)}\")\n",
    "print(f\"Zeitraum: {df_viz['spokes_ts'].min()} bis {df_viz['spokes_ts'].max()}\")\n"
   ],
   "id": "3799b93b768f08d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " ### 3.2 Aktivitätsverteilung nach Niederschlagskategorie\n",
    "\n",
    "Diese Visualisierung zeigt, wie sich die Aktivitätszustände der Tiere (Ruhen, Gehen, Grasen, Rennen)\n",
    "je nach Niederschlagskategorie verteilen. So können wir erkennen, ob Tiere bei Regen ihr Verhalten ändern.\n"
   ],
   "id": "69d08798983ae307"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Aktivitätsverteilung nach Niederschlagskategorie\n",
    "# Daten aggregieren: Durchschnittliche Minuten pro Aktivität nach Regenkategorie\n",
    "\n",
    "activity_cols = ['state_resting', 'state_walking', 'state_grazing', 'state_running']\n",
    "\n",
    "# Gruppieren nach Regenkategorie und Mittelwerte berechnen\n",
    "activity_by_rain = df_viz.groupby('rain_category')[activity_cols].mean()\n",
    "\n",
    "# Regenkategorien sortieren (von kein bis sehr_stark)\n",
    "rain_order = ['kein', 'sehr_schwach', 'schwach', 'mässig', 'stark', 'sehr_stark']\n",
    "activity_by_rain = activity_by_rain.reindex([cat for cat in rain_order if cat in activity_by_rain.index])\n",
    "\n",
    "# Prozentuale Verteilung je Kategorie berechnen\n",
    "activity_pct = activity_by_rain.div(activity_by_rain.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Stacked Bar Chart (Prozentwerte)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "# Manuelles Zeichnen, um Kontrolle über Labels zu haben\n",
    "bottom = None\n",
    "bars = []\n",
    "for i, col in enumerate(activity_cols):\n",
    "    values = activity_pct[col].values\n",
    "    b = ax.bar(activity_pct.index, values, bottom=bottom, color=colors[i], width=0.7, label=col)\n",
    "    bars.append(b)\n",
    "    bottom = values if bottom is None else bottom + values\n",
    "\n",
    "# Prozentlabels auf jedem Segment anzeigen (nur wenn > 3%)\n",
    "for i, col in enumerate(activity_cols):\n",
    "    for rect, val in zip(bars[i], activity_pct[col].values):\n",
    "        if val >= 3:  # kleine Segmente nicht beschriften, um Clutter zu vermeiden\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                rect.get_y() + rect.get_height() / 2,\n",
    "                f\"{val:.0f}%\",\n",
    "                ha='center', va='center', fontsize=9, color='white', fontweight='bold'\n",
    "            )\n",
    "\n",
    "ax.set_title('Aktivitätsverteilung der Tiere nach Niederschlagskategorie (in %)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Niederschlagskategorie', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Anteil (%)', fontsize=12, fontweight='bold')\n",
    "ax.legend(['Ruhen', 'Gehen', 'Grasen', 'Rennen'], title='Aktivität', loc='upper right', frameon=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Grid anpassen\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "ax.set_axisbelow(True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/processed/aktivitaetsverteilung.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistik ausgeben (Minuten + Prozent)\n",
    "print('\\n=== Durchschnittliche Aktivitätsdauer nach Regenkategorie (Minuten) ===')\n",
    "print(activity_by_rain.round(2))\n",
    "print('\\n=== Prozentuale Verteilung nach Regenkategorie (%) ===')\n",
    "print(activity_pct.round(1))"
   ],
   "id": "b36c8c8bbd79a88f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Zeitreihen-Analyse: Niederschlag und Aktivität über Zeit\n",
    "\n",
    "Diese Dual-Axis-Visualisierung zeigt den zeitlichen Verlauf von Niederschlag und Tieraktivität.\n",
    "Die blaue Linie zeigt die Niederschlagsmenge (mm/h), während die farbigen Bereiche die\n",
    "durchschnittliche Aktivitätsdauer darstellen.\n"
   ],
   "id": "f97cee2a461a3ce4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Zeitreihen-Analyse: Niederschlag und Aktivität über Zeit\n",
    "\n",
    "# Daten nach Stunden aggregieren für bessere Übersichtlichkeit\n",
    "df_viz['hour'] = pd.to_datetime(df_viz['spokes_ts']).dt.floor('H')\n",
    "\n",
    "# Gruppieren nach Stunde\n",
    "hourly_data = df_viz.groupby('hour').agg({\n",
    "    'rain_h': 'mean',\n",
    "    'state_resting': 'mean',\n",
    "    'state_walking': 'mean',\n",
    "    'state_grazing': 'mean',\n",
    "    'state_running': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Dual-Axis Plot\n",
    "fig, ax1 = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Niederschlag (linke Y-Achse)\n",
    "color_rain = '#1E90FF'\n",
    "ax1.set_xlabel('Zeitpunkt', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Niederschlag (mm/h)', fontsize=12, fontweight='bold', color=color_rain)\n",
    "ax1.fill_between(hourly_data['hour'], hourly_data['rain_h'],\n",
    "                 alpha=0.3, color=color_rain, label='Niederschlag')\n",
    "ax1.plot(hourly_data['hour'], hourly_data['rain_h'],\n",
    "         color=color_rain, linewidth=2, label='_nolegend_')\n",
    "ax1.tick_params(axis='y', labelcolor=color_rain)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Aktivitäten (rechte Y-Achse)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Durchschnittliche Aktivitätsdauer (Minuten)',\n",
    "               fontsize=12, fontweight='bold')\n",
    "\n",
    "# Aktivitätslinien plotten\n",
    "ax2.plot(hourly_data['hour'], hourly_data['state_resting'],\n",
    "         color='#2E86AB', linewidth=2, label='Ruhen', marker='o', markersize=3)\n",
    "ax2.plot(hourly_data['hour'], hourly_data['state_walking'],\n",
    "         color='#A23B72', linewidth=2, label='Gehen', marker='s', markersize=3)\n",
    "ax2.plot(hourly_data['hour'], hourly_data['state_grazing'],\n",
    "         color='#F18F01', linewidth=2, label='Grasen', marker='^', markersize=3)\n",
    "ax2.plot(hourly_data['hour'], hourly_data['state_running'],\n",
    "         color='#C73E1D', linewidth=2, label='Rennen', marker='d', markersize=3)\n",
    "\n",
    "# Titel und Legende\n",
    "plt.title('Zeitlicher Verlauf: Niederschlag und Tieraktivität',\n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Legenden kombinieren\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2,\n",
    "           loc='upper left', frameon=True, fontsize=10)\n",
    "\n",
    "# X-Achse formatieren\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('data/processed/zeitliche_muster.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Korrelationsanalyse\n",
    "print(\"\\n=== Korrelation: Niederschlag ↔ Aktivität ===\")\n",
    "corr_data = df_viz[['rain_h', 'state_resting', 'state_walking', 'state_grazing', 'state_running']].corr()\n",
    "print(corr_data['rain_h'].drop('rain_h').round(3))\n"
   ],
   "id": "6158fd6035c61c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpretation der Ergebnisse\n",
    "\n",
    "**Aktivitätsverteilung nach Niederschlag:**\n",
    "- Die gestapelte Balkenvisualisierung zeigt deutlich, wie sich die Verhaltensweisen bei unterschiedlichen Niederschlagsintensitäten verändern\n",
    "- Wichtige Frage: Ruhen Tiere mehr bei starkem Regen? Grasen sie weniger?\n",
    "\n",
    "**Zeitliche Muster:**\n",
    "- Die Dual-Axis-Darstellung ermöglicht den direkten visuellen Vergleich zwischen Wetterereignissen und Verhaltensänderungen\n",
    "- Zeitliche Verzögerungen zwischen Niederschlag und Verhaltensänderungen können erkannt werden\n",
    "\n",
    "**Korrelationswerte:**\n",
    "- Positive Werte: Aktivität steigt mit Niederschlag\n",
    "- Negative Werte: Aktivität sinkt mit Niederschlag\n",
    "- Werte nahe 0: Kein linearer Zusammenhang\n"
   ],
   "id": "4ea1a713d68405d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "time_anlysies = time.time() - start_analyzis\n",
    "print(f\"Datenanalyse-Verarbeitung: {time_anlysies:.2f} Sekunden\")"
   ],
   "id": "79b944032d632fe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Performance-Messungen",
   "id": "91712929174161cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n+------------------------------+-----------+\")\n",
    "print(\"| Schritt                      | Zeit (s)  |\")\n",
    "print(\"+------------------------------+-----------+\")\n",
    "print(f\"| Daten-Verarbeitung           | {time_dataload:9.2f} |\")\n",
    "print(f\"| Join-Verarbeitung            | {time_joins:9.2f} |\")\n",
    "print(f\"| Datenanalyse-Verarbeitung    | {time_anlysies:9.2f} |\")\n",
    "print(\"+------------------------------+-----------+\")\n",
    "print(f\"| Job-Verarbeitung (Total)     | {time.time() - start_job:9.2f} |\")\n",
    "print(\"+------------------------------+-----------+\\n\")\n"
   ],
   "id": "2b8524096b4ebd79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Nur auf ZHAW-Cluster nötig\n",
    "#sc.stop()"
   ],
   "id": "2cb7ac7cb82daabe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
