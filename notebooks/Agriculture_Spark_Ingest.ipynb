{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dac9bb1",
   "metadata": {},
   "source": [
    "# Agriculture Spark – Ingest (CSV → Parquet)\n",
    "\n",
    "Dieses Notebook:\n",
    "- erstellt eine **SparkSession**\n",
    "- definiert **Schemas** für Hub/Spoke\n",
    "- liest die Original-CSV Dateien ein\n",
    "- führt **Parsing/Cleansing** durch (z.B. `\"-\"` → `null`, Timestamp-Parsing, Feature-Vector-Parsing)\n",
    "- speichert die bereinigten Daten als **Parquet**\n",
    "\n",
    "> **Hinweis:** Pfade sind standardmäßig auf die hochgeladenen Dateien in dieser Umgebung gesetzt. Wenn ihr das Notebook in eurem Repo/Cluster nutzt, passt die Pfade im nächsten Cell an.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dc103fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:09.081019Z",
     "start_time": "2025-12-20T09:29:09.074098Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 0) Imports & Pfade\n",
    "# =========================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import os\n",
    "\n",
    "# --- INPUT: Originaldateien ---\n",
    "HUB_CSV   = \"data/raw/dm-hub.csv\"\n",
    "SPOKE_CSV = \"data/raw/dm-spoke.csv\"\n",
    "\n",
    "# --- OUTPUT: Parquet-Zielordner ---\n",
    "OUT_ROOT = \"data/processed\"\n",
    "HUB_OUT_PARQUET   = os.path.join(OUT_ROOT, \"hub\")\n",
    "SPOKE_OUT_PARQUET = os.path.join(OUT_ROOT, \"spoke\")\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "print(\"HUB_CSV:\", HUB_CSV)\n",
    "print(\"SPOKE_CSV:\", SPOKE_CSV)\n",
    "print(\"OUT_ROOT:\", OUT_ROOT)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUB_CSV: data/raw/dm-hub.csv\n",
      "SPOKE_CSV: data/raw/dm-spoke.csv\n",
      "OUT_ROOT: data/processed\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "d37e36af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:09.133535Z",
     "start_time": "2025-12-20T09:29:09.121744Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 1) Spark Session erstellen\n",
    "# =========================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AgricultureSpark-Ingest\")\n",
    "    # sinnvolle Defaults (könnt ihr je nach Cluster anpassen)\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.7'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "1489f88b",
   "metadata": {},
   "source": [
    "## 2) Schemas definieren\n",
    "\n",
    "Wir erzwingen ein Schema statt alles als String einzulesen.  \n",
    "CSV-Felder, die manchmal `\"-\"` enthalten, werden zunächst als `string` eingelesen und danach sauber gecastet (damit kein Parsing crasht).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aadfb050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:09.169552Z",
     "start_time": "2025-12-20T09:29:09.163213Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 2) Schemas definieren\n",
    "# =========================\n",
    "# Hub: 7 Spalten (kein Header in CSV)\n",
    "# Format: <hubtracker> <timestamp> hub-coords <latitude> <longitude> <voltage/V> <temperature/°C>\n",
    "hub_schema_raw = T.StructType([\n",
    "    T.StructField(\"hubtracker\", T.StringType(), True),\n",
    "    T.StructField(\"timestamp\", T.StringType(), True),\n",
    "    T.StructField(\"hub_coords\", T.StringType(), True),\n",
    "    T.StructField(\"latitude\", T.StringType(), True),\n",
    "    T.StructField(\"longitude\", T.StringType(), True),\n",
    "    T.StructField(\"voltage\", T.StringType(), True),\n",
    "    T.StructField(\"temperature\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "# Spoke: 14 Spalten (kein Header in CSV)\n",
    "# Format: <hubtracker> <timestamp> spoke-visibility <spoketracker> <rssi/dB> <device-state>\n",
    "#         <voltage/V> <temperature/°C> <animal-state> <state-resting/min> <state-walking/min>\n",
    "#         <state-grazing/min> <state-running/min> <features-vector/15 values>\n",
    "spoke_schema_raw = T.StructType([\n",
    "    T.StructField(\"hubtracker\", T.StringType(), True),\n",
    "    T.StructField(\"timestamp\", T.StringType(), True),\n",
    "    T.StructField(\"spoke_visibility\", T.StringType(), True),\n",
    "    T.StructField(\"spoketracker\", T.StringType(), True),\n",
    "    T.StructField(\"rssi\", T.StringType(), True),\n",
    "    T.StructField(\"device_state\", T.StringType(), True),\n",
    "    T.StructField(\"voltage\", T.StringType(), True),\n",
    "    T.StructField(\"temperature\", T.StringType(), True),\n",
    "    T.StructField(\"animal_state\", T.StringType(), True),\n",
    "    T.StructField(\"state_resting\", T.StringType(), True),\n",
    "    T.StructField(\"state_walking\", T.StringType(), True),\n",
    "    T.StructField(\"state_grazing\", T.StringType(), True),\n",
    "    T.StructField(\"state_running\", T.StringType(), True),\n",
    "    T.StructField(\"features\", T.StringType(), True),     # features[15] als eine Spalte\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "b4e56c69",
   "metadata": {},
   "source": [
    "## 3) CSV einlesen (robust)\n",
    "\n",
    "Wir setzen:\n",
    "- `header=True`\n",
    "- `multiLine=True` (falls es Zeilenumbrüche in Quotes gibt)\n",
    "- `escape`/`quote` robust\n",
    "- `mode=PERMISSIVE` + `_corrupt_record` zur Diagnose\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f138e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:09.451580Z",
     "start_time": "2025-12-20T09:29:09.180970Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 3) CSV einlesen\n",
    "# =========================\n",
    "csv_options = {\n",
    "    \"header\": \"false\",\n",
    "    \"sep\": \",\",\n",
    "    \"quote\": '\"',\n",
    "    \"escape\": '\"',\n",
    "    \"multiLine\": \"true\",\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "    \"columnNameOfCorruptRecord\": \"_corrupt_record\",\n",
    "    \"encoding\": \"UTF-8\",\n",
    "}\n",
    "\n",
    "# HUB: mit Schema lesen (keine Header-Zeile)\n",
    "hub_raw = spark.read.schema(hub_schema_raw).options(**csv_options).csv(HUB_CSV)\n",
    "\n",
    "print(\"HUB columns:\", hub_raw.columns)\n",
    "hub_raw.show(5, truncate=False)\n",
    "\n",
    "# SPOKE: mit Schema lesen (keine Header-Zeile)\n",
    "spoke_raw = spark.read.schema(spoke_schema_raw).options(**csv_options).csv(SPOKE_CSV)\n",
    "\n",
    "print(\"SPOKE columns:\", spoke_raw.columns)\n",
    "spoke_raw.show(5, truncate=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUB columns: ['hubtracker', 'timestamp', 'hub_coords', 'latitude', 'longitude', 'voltage', 'temperature']\n",
      "+----------+----------+----------+----------+---------+-------+-----------+\n",
      "|hubtracker|timestamp |hub_coords|latitude  |longitude|voltage|temperature|\n",
      "+----------+----------+----------+----------+---------+-------+-----------+\n",
      "|937832    |1752294998|hub-coords|46.3694617|7.6737851|-      |-          |\n",
      "|932400    |1752295123|hub-coords|46.3693767|7.6737784|-      |-          |\n",
      "|937832    |1752294998|hub-coords|46.3694617|7.6737851|4.79   |11         |\n",
      "|937832    |1752294998|hub-coords|46.3694617|7.6737851|4.79   |11         |\n",
      "|937832    |1752294998|hub-coords|46.3694617|7.6737851|4.79   |11         |\n",
      "+----------+----------+----------+----------+---------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "SPOKE columns: ['hubtracker', 'timestamp', 'spoke_visibility', 'spoketracker', 'rssi', 'device_state', 'voltage', 'temperature', 'animal_state', 'state_resting', 'state_walking', 'state_grazing', 'state_running', 'features']\n",
      "+----------+----------+----------------+-----------------+----+------------+-------+-----------+------------+-------------+-------------+-------------+-------------+--------+\n",
      "|hubtracker|timestamp |spoke_visibility|spoketracker     |rssi|device_state|voltage|temperature|animal_state|state_resting|state_walking|state_grazing|state_running|features|\n",
      "+----------+----------+----------------+-----------------+----+------------+-------+-----------+------------+-------------+-------------+-------------+-------------+--------+\n",
      "|937832    |1752295249|spoke-visibility|newspoke-EFD1CF8E|-98 |noproblem   |4.55   |13         |running     |1            |53           |198          |6            |59      |\n",
      "|937832    |1752295249|spoke-visibility|newspoke-C99FA3B7|-86 |noproblem   |4.55   |15         |grazing     |191          |191          |245          |0            |68      |\n",
      "|937832    |1752295249|spoke-visibility|newspoke-42EB4ABA|-101|noproblem   |4.55   |14         |walking     |92           |120          |57           |5            |53      |\n",
      "|937832    |1752295249|spoke-visibility|newspoke-6AA669CF|-100|noproblem   |4.55   |15         |running     |86           |145          |11           |2            |62      |\n",
      "|937832    |1752295249|spoke-visibility|newspoke-C9CBB7DE|-81 |highactivity|4.55   |21         |walking     |118          |25           |89           |4            |74      |\n",
      "+----------+----------+----------------+-----------------+----+------------+-------+-----------+------------+-------------+-------------+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "4d2a9bb0",
   "metadata": {},
   "source": [
    "## 4) Parsing / Cleansing\n",
    "\n",
    "### Hub\n",
    "- `timestamp` → Timestamp (`event_ts`)\n",
    "- `latitude/longitude` → Double\n",
    "- `voltage/temperature/signal` (`\"-\"` → `null`) → Double\n",
    "- optional `delay` falls vorhanden\n",
    "\n",
    "### Spoke\n",
    "- `timestamp` → Timestamp\n",
    "- numerische Felder casten\n",
    "- Feature-Vektor (`features`) robust splitten und auf **15 Features** bringen\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1fa6b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:10.021921Z",
     "start_time": "2025-12-20T09:29:09.475416Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 4A) HUB Parsing\n",
    "# =========================\n",
    "\n",
    "def dash_to_double(colname: str):\n",
    "    return F.when(F.col(colname).isin(\"-\", \"\"), F.lit(None)).otherwise(F.col(colname)).cast(\"double\")\n",
    "\n",
    "def dash_to_int(colname: str):\n",
    "    return F.when(F.col(colname).isin(\"-\", \"\"), F.lit(None)).otherwise(F.col(colname)).cast(\"int\")\n",
    "\n",
    "# Timestamp-Parsing: mehrere Formate (Spark versucht ISO automatisch)\n",
    "hub = hub_raw.withColumn(\n",
    "    \"event_ts\",\n",
    "    F.coalesce(\n",
    "        F.to_timestamp(\"timestamp\"),\n",
    "        F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        F.to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"),\n",
    "        F.to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ssX\"),\n",
    "    )\n",
    ").withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    "\n",
    "# Casting\n",
    "hub = (\n",
    "    hub\n",
    "    .withColumn(\"latitude_d\",  dash_to_double(\"latitude\"))\n",
    "    .withColumn(\"longitude_d\", dash_to_double(\"longitude\"))\n",
    "    .withColumn(\"voltage_d\",   dash_to_double(\"voltage\"))\n",
    "    .withColumn(\"temperature_d\", dash_to_double(\"temperature\"))\n",
    ")\n",
    "\n",
    "hub_parsed = hub.select(\n",
    "    \"hubtracker\",\n",
    "    \"event_ts\",\n",
    "    \"event_date\",\n",
    "    \"hub_coords\",\n",
    "    F.col(\"latitude_d\").alias(\"latitude\"),\n",
    "    F.col(\"longitude_d\").alias(\"longitude\"),\n",
    "    F.col(\"voltage_d\").alias(\"voltage\"),\n",
    "    F.col(\"temperature_d\").alias(\"temperature\"),\n",
    "    \"_corrupt_record\" if \"_corrupt_record\" in hub.columns else F.lit(None).alias(\"_corrupt_record\"),\n",
    ")\n",
    "\n",
    "hub_parsed.show(5, truncate=False)\n",
    "hub_parsed.printSchema()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+----------+----------+---------+-------+-----------+---------------+\n",
      "|hubtracker|event_ts|event_date|hub_coords|latitude  |longitude|voltage|temperature|_corrupt_record|\n",
      "+----------+--------+----------+----------+----------+---------+-------+-----------+---------------+\n",
      "|937832    |NULL    |NULL      |hub-coords|46.3694617|7.6737851|NULL   |NULL       |NULL           |\n",
      "|932400    |NULL    |NULL      |hub-coords|46.3693767|7.6737784|NULL   |NULL       |NULL           |\n",
      "|937832    |NULL    |NULL      |hub-coords|46.3694617|7.6737851|4.79   |11.0       |NULL           |\n",
      "|937832    |NULL    |NULL      |hub-coords|46.3694617|7.6737851|4.79   |11.0       |NULL           |\n",
      "|937832    |NULL    |NULL      |hub-coords|46.3694617|7.6737851|4.79   |11.0       |NULL           |\n",
      "+----------+--------+----------+----------+----------+---------+-------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- hubtracker: string (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- hub_coords: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- _corrupt_record: void (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "cb7aee9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:11.143532Z",
     "start_time": "2025-12-20T09:29:10.041343Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 4B) SPOKE Parsing inkl. Feature-Vektor\n",
    "# =========================\n",
    "\n",
    "spoke = (\n",
    "    spoke_raw\n",
    "    .withColumn(\n",
    "        \"event_ts\",\n",
    "        F.coalesce(\n",
    "            F.to_timestamp(\"timestamp\"),\n",
    "            F.to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            F.to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"),\n",
    "            F.to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ssX\"),\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_ts\"))\n",
    ")\n",
    "\n",
    "# Numerik casten\n",
    "spoke = (\n",
    "    spoke\n",
    "    .withColumn(\"spoke_visibility_i\", dash_to_int(\"spoke_visibility\"))\n",
    "    .withColumn(\"rssi_i\", dash_to_int(\"rssi\"))\n",
    "    .withColumn(\"device_state_i\", dash_to_int(\"device_state\"))\n",
    "    .withColumn(\"voltage_d\", dash_to_double(\"voltage\"))\n",
    "    .withColumn(\"temperature_d\", dash_to_double(\"temperature\"))\n",
    "    .withColumn(\"animal_state_i\", dash_to_int(\"animal_state\"))\n",
    "    .withColumn(\"state_resting_i\", dash_to_int(\"state_resting\"))\n",
    "    .withColumn(\"state_walking_i\", dash_to_int(\"state_walking\"))\n",
    "    .withColumn(\"state_grazing_i\", dash_to_int(\"state_grazing\"))\n",
    "    .withColumn(\"state_running_i\", dash_to_int(\"state_running\"))\n",
    ")\n",
    "\n",
    "# Feature parsing:\n",
    "# - entferne [ ]\n",
    "# - normalisiere ; -> ,\n",
    "# - split\n",
    "features_clean = F.regexp_replace(F.col(\"features\"), \";\", \",\")\n",
    "features_clean = F.regexp_replace(features_clean, r\"\\[|\\]\", \"\")\n",
    "features_arr = F.split(features_clean, \",\")\n",
    "\n",
    "spoke = spoke.withColumn(\"features_arr_raw\", features_arr)\n",
    "spoke = spoke.withColumn(\"feature_count\", F.size(\"features_arr_raw\"))\n",
    "\n",
    "# Cast elements safely\n",
    "spoke = spoke.withColumn(\n",
    "    \"features_arr\",\n",
    "    F.expr(\"\"\"\n",
    "        transform(features_arr_raw, x ->\n",
    "            CASE\n",
    "                WHEN x IS NULL THEN NULL\n",
    "                WHEN trim(x) = '' THEN NULL\n",
    "                WHEN trim(x) = '-' THEN NULL\n",
    "                ELSE cast(trim(x) as double)\n",
    "            END\n",
    "        )\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# Pad/trim to exactly 15\n",
    "spoke = spoke.withColumn(\n",
    "    \"features_15\",\n",
    "    F.when(\n",
    "        F.col(\"features_arr\").isNull(),\n",
    "        F.array_repeat(F.lit(None).cast(\"double\"), 15)\n",
    "    ).otherwise(\n",
    "        F.when(\n",
    "            F.size(\"features_arr\") >= 15,\n",
    "            F.slice(\"features_arr\", 1, 15)\n",
    "        ).otherwise(\n",
    "            F.concat(\n",
    "                F.col(\"features_arr\"),\n",
    "                F.array_repeat(F.lit(None).cast(\"double\"), 15 - F.size(\"features_arr\"))\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "spoke = spoke.withColumn(\"feature_ok\", F.col(\"feature_count\") >= F.lit(15))\n",
    "\n",
    "# explode into f01..f15\n",
    "for i in range(15):\n",
    "    spoke = spoke.withColumn(f\"f{i+1:02d}\", F.col(\"features_15\")[i])\n",
    "\n",
    "spoke_parsed = spoke.select(\n",
    "    \"hubtracker\",\n",
    "    \"spoketracker\",\n",
    "    \"event_ts\",\n",
    "    \"event_date\",\n",
    "    F.col(\"spoke_visibility_i\").alias(\"spoke_visibility\"),\n",
    "    F.col(\"rssi_i\").alias(\"rssi\"),\n",
    "    F.col(\"device_state_i\").alias(\"device_state\"),\n",
    "    F.col(\"voltage_d\").alias(\"voltage\"),\n",
    "    F.col(\"temperature_d\").alias(\"temperature\"),\n",
    "    F.col(\"animal_state_i\").alias(\"animal_state\"),\n",
    "    F.col(\"state_resting_i\").alias(\"state_resting\"),\n",
    "    F.col(\"state_walking_i\").alias(\"state_walking\"),\n",
    "    F.col(\"state_grazing_i\").alias(\"state_grazing\"),\n",
    "    F.col(\"state_running_i\").alias(\"state_running\"),\n",
    "    \"feature_count\",\n",
    "    \"feature_ok\",\n",
    "    *[f\"f{i+1:02d}\" for i in range(15)],\n",
    ")\n",
    "\n",
    "spoke_parsed.show(5, truncate=False)\n",
    "spoke_parsed.printSchema()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------+----------+----------------+----+------------+-------+-----------+------------+-------------+-------------+-------------+-------------+-------------+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|hubtracker|spoketracker     |event_ts|event_date|spoke_visibility|rssi|device_state|voltage|temperature|animal_state|state_resting|state_walking|state_grazing|state_running|feature_count|feature_ok|f01 |f02 |f03 |f04 |f05 |f06 |f07 |f08 |f09 |f10 |f11 |f12 |f13 |f14 |f15 |\n",
      "+----------+-----------------+--------+----------+----------------+----+------------+-------+-----------+------------+-------------+-------------+-------------+-------------+-------------+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|937832    |newspoke-EFD1CF8E|NULL    |NULL      |NULL            |-98 |NULL        |4.55   |13.0       |NULL        |1            |53           |198          |6            |1            |false     |59.0|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|\n",
      "|937832    |newspoke-C99FA3B7|NULL    |NULL      |NULL            |-86 |NULL        |4.55   |15.0       |NULL        |191          |191          |245          |0            |1            |false     |68.0|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|\n",
      "|937832    |newspoke-42EB4ABA|NULL    |NULL      |NULL            |-101|NULL        |4.55   |14.0       |NULL        |92           |120          |57           |5            |1            |false     |53.0|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|\n",
      "|937832    |newspoke-6AA669CF|NULL    |NULL      |NULL            |-100|NULL        |4.55   |15.0       |NULL        |86           |145          |11           |2            |1            |false     |62.0|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|\n",
      "|937832    |newspoke-C9CBB7DE|NULL    |NULL      |NULL            |-81 |NULL        |4.55   |21.0       |NULL        |118          |25           |89           |4            |1            |false     |74.0|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|\n",
      "+----------+-----------------+--------+----------+----------------+----+------------+-------+-----------+------------+-------------+-------------+-------------+-------------+-------------+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- hubtracker: string (nullable = true)\n",
      " |-- spoketracker: string (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- spoke_visibility: integer (nullable = true)\n",
      " |-- rssi: integer (nullable = true)\n",
      " |-- device_state: integer (nullable = true)\n",
      " |-- voltage: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- animal_state: integer (nullable = true)\n",
      " |-- state_resting: integer (nullable = true)\n",
      " |-- state_walking: integer (nullable = true)\n",
      " |-- state_grazing: integer (nullable = true)\n",
      " |-- state_running: integer (nullable = true)\n",
      " |-- feature_count: integer (nullable = false)\n",
      " |-- feature_ok: boolean (nullable = false)\n",
      " |-- f01: double (nullable = true)\n",
      " |-- f02: double (nullable = true)\n",
      " |-- f03: double (nullable = true)\n",
      " |-- f04: double (nullable = true)\n",
      " |-- f05: double (nullable = true)\n",
      " |-- f06: double (nullable = true)\n",
      " |-- f07: double (nullable = true)\n",
      " |-- f08: double (nullable = true)\n",
      " |-- f09: double (nullable = true)\n",
      " |-- f10: double (nullable = true)\n",
      " |-- f11: double (nullable = true)\n",
      " |-- f12: double (nullable = true)\n",
      " |-- f13: double (nullable = true)\n",
      " |-- f14: double (nullable = true)\n",
      " |-- f15: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "221f17bf",
   "metadata": {},
   "source": [
    "## 5) Speicherung als Parquet\n",
    "\n",
    "Wir speichern partitioniert nach `event_date`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b0e67f7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-20T09:29:11.451810Z",
     "start_time": "2025-12-20T09:29:11.212165Z"
    }
   },
   "source": [
    "# =========================\n",
    "# 5) Write Parquet\n",
    "# =========================\n",
    "(\n",
    "    hub_parsed\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"event_date\")\n",
    "    .parquet(HUB_OUT_PARQUET)\n",
    ")\n",
    "\n",
    "(\n",
    "    spoke_parsed\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"event_date\")\n",
    "    .parquet(SPOKE_OUT_PARQUET)\n",
    ")\n",
    "\n",
    "print(\"Wrote hub parquet to:\", HUB_OUT_PARQUET)\n",
    "print(\"Wrote spoke parquet to:\", SPOKE_OUT_PARQUET)\n"
   ],
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The Parquet datasource doesn't support the column `_corrupt_record` of the type \"VOID\".",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAnalysisException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[32]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# 5) Write Parquet\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# =========================\u001B[39;00m\n\u001B[32m      4\u001B[39m (\n\u001B[32m      5\u001B[39m     \u001B[43mhub_parsed\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43moverwrite\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mevent_date\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[43m    \u001B[49m\u001B[43m.\u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mHUB_OUT_PARQUET\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m )\n\u001B[32m     12\u001B[39m (\n\u001B[32m     13\u001B[39m     spoke_parsed\n\u001B[32m     14\u001B[39m     .write\n\u001B[32m   (...)\u001B[39m\u001B[32m     17\u001B[39m     .parquet(SPOKE_OUT_PARQUET)\n\u001B[32m     18\u001B[39m )\n\u001B[32m     20\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mWrote hub parquet to:\u001B[39m\u001B[33m\"\u001B[39m, HUB_OUT_PARQUET)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\dev\\pycharm\\agriculture-spark\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001B[39m, in \u001B[36mDataFrameWriter.parquet\u001B[39m\u001B[34m(self, path, mode, partitionBy, compression)\u001B[39m\n\u001B[32m   1719\u001B[39m     \u001B[38;5;28mself\u001B[39m.partitionBy(partitionBy)\n\u001B[32m   1720\u001B[39m \u001B[38;5;28mself\u001B[39m._set_opts(compression=compression)\n\u001B[32m-> \u001B[39m\u001B[32m1721\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\dev\\pycharm\\agriculture-spark\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\dev\\pycharm\\agriculture-spark\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    181\u001B[39m converted = convert_exception(e.java_exception)\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[32m    183\u001B[39m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[32m    184\u001B[39m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m185\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    186\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    187\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAnalysisException\u001B[39m: [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The Parquet datasource doesn't support the column `_corrupt_record` of the type \"VOID\"."
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "f26a19e8",
   "metadata": {},
   "source": [
    "## 6) Read-back Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d424b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_pq = spark.read.parquet(HUB_OUT_PARQUET)\n",
    "spoke_pq = spark.read.parquet(SPOKE_OUT_PARQUET)\n",
    "\n",
    "print(\"hub_pq rows:\", hub_pq.count(), \"columns:\", len(hub_pq.columns))\n",
    "print(\"spoke_pq rows:\", spoke_pq.count(), \"columns:\", len(spoke_pq.columns))\n",
    "\n",
    "hub_pq.groupBy(\"event_date\").count().orderBy(\"event_date\").show(10, truncate=False)\n",
    "spoke_pq.groupBy(\"event_date\").count().orderBy(\"event_date\").show(10, truncate=False)\n",
    "\n",
    "spoke_pq.select(\n",
    "    F.count(\"*\").alias(\"n\"),\n",
    "    F.sum(F.col(\"feature_ok\").cast(\"int\")).alias(\"n_feature_ok\"),\n",
    "    F.avg(F.col(\"feature_ok\").cast(\"int\")).alias(\"share_feature_ok\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6bf55",
   "metadata": {},
   "source": [
    "## 7) Stop Spark (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
